{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project: I94 Immigration\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The US National Tourism and Trade Office wants to set up and store their immigration data on the any cloud service that supports quick ingesting, transforming and loading data to their reports as well as other system for prompt tracking for people who come and leave the US.\n",
    "Besides, these data will help them to have an eye on what relation foreigners and immigrants might have on other aspects, such as US Cities Demographic, US Airports in each region,...\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import configparser\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Connect to AWS Credentials\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "1. What Data?\n",
    "- I94 Immigration Data\n",
    "- U.S. City Demographic Data\n",
    "- Airport Code Table\n",
    "\n",
    "2. What tool?\n",
    "- Main data format is SAS and expect to be grow in the future so we will use Spark to ingest and transform data\n",
    "- After data is transformed, to support prompt writing and reading, Spark will load processed data into S3 Data Lake, data are queried in-place on the S3 parquet data.\n",
    "\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "- I94 Immigration Data: This data comes from the US National Tourism and Trade Office. This will be the main data source used for analysis. Data will be in SAS format\n",
    "    - **Data Dictionary** :All column definition and sample data is stored in this file: [I94_SAS_Labels_Descriptions](I94_SAS_Labels_Descriptions.SAS)\n",
    "- U.S. City Demographic Data: This data comes from OpenSoft, including detailed information about US Cities like age, population, Race,...\n",
    "- Airport Code Table: This is a simple table of airport codes and corresponding cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "# raw_df = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE Saving file in parquet format\n",
      "DONE Reading Immigration data\n",
      "DONE Reading Airport data\n",
      "DONE Reading US Cities data\n"
     ]
    }
   ],
   "source": [
    "#write to parquet\n",
    "# raw_df.write.parquet(\"i94_data\")\n",
    "print(\"DONE Saving file in parquet format\")\n",
    "\n",
    "immigration_df=spark.read.parquet(\"i94_data\")\n",
    "print(\"DONE Reading Immigration data\")\n",
    "\n",
    "airport_df = spark.read.format(\"csv\").option(\"header\", \"true\").load('airport-codes_csv.csv')\n",
    "print(\"DONE Reading Airport data\")\n",
    "\n",
    "uscity_df = spark.read.options(header='true', delimiter=\";\").csv(\"us-cities-demographics.csv\")\n",
    "print(\"DONE Reading US Cities data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration df length: 3096313\n",
      "Airport df length: 55075\n",
      "US Cities df length: 2891\n"
     ]
    }
   ],
   "source": [
    "print(f'Immigration df length: {immigration_df.count()}')\n",
    "print(f'Airport df length: {airport_df.count()}')\n",
    "print(f'US Cities df length: {uscity_df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|      1|      3|      8|     0|      0|    0|       1|      76|  100|      0|      8|     98|      8|      0|      0|    27|   100|      2|     0|    1|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploring data\n",
    "\n",
    "# Count NULLs in each column in Immigration DF\n",
    "immigration_df.select([count(when(isnull(c), c)).alias(c) for c in immigration_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- After checking NULLs, we will not take those columns with too many NULLs into transforming and analyzing: isnum, entdepu, entdepd, occup\n",
    "- Next, choose critical columns for fact_immigration table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|ident|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|    0|   0|   0|        7006|        0|          0|         0|        5676|   14045|    45886|     26389|          0|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count NULLs in each column in airport_df\n",
    "airport_df.select([count(when(isnull(c), c)).alias(c) for c in airport_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|City|State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Race|Count|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|   0|    0|         0|              3|                3|               0|                13|          13|                    16|         0|   0|    0|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uscity_df.select([count(when(isnull(c), c)).alias(c) for c in uscity_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "[Conceptual Data Model](DataModel.png)\n",
    "<img src=\"DataModel.png\"/>\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+---------+------+-----------+-------------+----------------+-------+-------+--------+\n",
      "|cicid| i94yr|i94mon|CountryID|PortID|ArrivalDate|DepartureDate|DestinationState|i94mode|VisaCat|visatype|\n",
      "+-----+------+------+---------+------+-----------+-------------+----------------+-------+-------+--------+\n",
      "| 67.0|2016.0|   4.0|    103.0|   ATL|    20545.0|      20580.0|              AL|    1.0|    2.0|      WT|\n",
      "| 70.0|2016.0|   4.0|    103.0|   ATL|    20545.0|      20567.0|              FL|    1.0|    2.0|      WT|\n",
      "| 69.0|2016.0|   4.0|    103.0|   ATL|    20545.0|      20560.0|              FL|    1.0|    2.0|      WT|\n",
      "|  7.0|2016.0|   4.0|    276.0|   ATL|    20551.0|         null|              AL|    1.0|    3.0|      F1|\n",
      "|124.0|2016.0|   4.0|    103.0|   NEW|    20545.0|      20554.0|              NJ|    1.0|    2.0|      WT|\n",
      "+-----+------+------+---------+------+-----------+-------------+----------------+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select columns and rename for better understanding. Then drop duplicates if any based on cicid column\n",
    "fact_immigration_df = immigration_df.select(\"cicid\"\\\n",
    "                                  ,\"i94yr\"\\\n",
    "                                  ,\"i94mon\"\\\n",
    "                                  ,col(\"i94res\").alias(\"CountryID\")\\\n",
    "                                  ,col(\"i94port\").alias(\"PortID\")\\\n",
    "                                  ,col(\"arrdate\").alias(\"ArrivalDate\")\\\n",
    "                                  ,col(\"depdate\").alias(\"DepartureDate\")\\\n",
    "                                  ,col(\"i94addr\").alias(\"DestinationState\")\\\n",
    "                                  ,\"i94mode\"\\\n",
    "                                  ,col(\"i94visa\").alias(\"VisaCat\")\\\n",
    "                                  ,\"visatype\").dropDuplicates([\"cicid\"])\n",
    "fact_immigration_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- CountryID: double (nullable = true)\n",
      " |-- PortID: string (nullable = true)\n",
      " |-- ArrivalDate: double (nullable = true)\n",
      " |-- DepartureDate: double (nullable = true)\n",
      " |-- DestinationState: string (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- VisaCat: double (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema\n",
    "fact_immigration_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#From the Schema, some dates columns should be convert to timestamp\n",
    "# Create a UDF to help convert date in sas numberic format to timestamp\n",
    "\n",
    "@udf(TimestampType())\n",
    "def sas_to_timestamp(x):\n",
    "    try:\n",
    "        if  x is not None:\n",
    "            epoch = datetime.datetime(1960, 1, 1)\n",
    "            return epoch + datetime.timedelta(days=int(float(x)))\n",
    "        else:\n",
    "            return pd.Timestamp('1900-1-1')\n",
    "    except:\n",
    "         return pd.Timestamp('1900-1-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+---------+------+-------------------+-------------------+----------------+-------+-------+--------+\n",
      "|cicid| i94yr|i94mon|CountryID|PortID|        ArrivalDate|      DepartureDate|DestinationState|i94mode|VisaCat|visatype|\n",
      "+-----+------+------+---------+------+-------------------+-------------------+----------------+-------+-------+--------+\n",
      "| 67.0|2016.0|   4.0|    103.0|   ATL|2016-04-01 00:00:00|2016-05-06 00:00:00|              AL|    1.0|    2.0|      WT|\n",
      "| 70.0|2016.0|   4.0|    103.0|   ATL|2016-04-01 00:00:00|2016-04-23 00:00:00|              FL|    1.0|    2.0|      WT|\n",
      "| 69.0|2016.0|   4.0|    103.0|   ATL|2016-04-01 00:00:00|2016-04-16 00:00:00|              FL|    1.0|    2.0|      WT|\n",
      "+-----+------+------+---------+------+-------------------+-------------------+----------------+-------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration_df = fact_immigration_df.withColumn(\"ArrivalDate\",sas_to_timestamp(col(\"ArrivalDate\")))\\\n",
    "                                        .withColumn(\"DepartureDate\",sas_to_timestamp(col(\"DepartureDate\")))\n",
    "\n",
    "fact_immigration_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------------------+-----------+--------+--------------------+\n",
      "|StateID|         type|                name|iso_country|gps_code|         coordinates|\n",
      "+-------+-------------+--------------------+-----------+--------+--------------------+\n",
      "|     ME|seaplane_base|Saint Peter's Sea...|         US|    01ME|-68.5002975463867...|\n",
      "|     AR|     heliport|Saline Memorial H...|         US|    04AR|-92.586047, 34.57...|\n",
      "|     NV|small_airport|Kingston Ranch Ai...|         US|    04NV|-115.665000916, 3...|\n",
      "+-------+-------------+--------------------+-----------+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_airport = airport_df.select(regexp_extract(col(\"iso_region\"), \".*-(.*)\",1).alias(\"StateID\")\\\n",
    "                          ,\"type\"\\\n",
    "                          ,\"name\"\\\n",
    "                          ,\"iso_country\"\\\n",
    "                          ,\"gps_code\"\\\n",
    "                          ,\"coordinates\"\n",
    "                         ).dropDuplicates()\n",
    "dim_airport.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+------+\n",
      "|cicid| Age|BirthYear|gender|\n",
      "+-----+----+---------+------+\n",
      "|  6.0|37.0|   1979.0|  null|\n",
      "|  7.0|25.0|   1991.0|     M|\n",
      "+-----+----+---------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_immigrants = immigration_df.select(\"cicid\"\\\n",
    "                            ,col(\"i94bir\").alias(\"Age\")\\\n",
    "                            ,col(\"biryear\").alias(\"BirthYear\")\\\n",
    "                            ,\"gender\"\n",
    "                            )\n",
    "dim_immigrants.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+----------+----------------+------------+------------------+-----+\n",
      "|State Code|        State|         City|Median Age|Total Population|Foreign-born|              Race|Count|\n",
      "+----------+-------------+-------------+----------+----------------+------------+------------------+-----+\n",
      "|        MD|     Maryland|Silver Spring|      33.8|           82463|       30908|Hispanic or Latino|25924|\n",
      "|        MA|Massachusetts|       Quincy|      41.0|           93629|       32935|             White|58723|\n",
      "+----------+-------------+-------------+----------+----------------+------------+------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_cities = uscity_df.select(\"State Code\"\\\n",
    "                        ,\"State\"\\\n",
    "                        ,\"City\"\\\n",
    "                        ,\"Median Age\"\\\n",
    "                        ,\"Total Population\" \\\n",
    "                        ,\"Foreign-born\"\\\n",
    "                        ,\"Race\"\\\n",
    "                        ,\"Count\"\n",
    "                        )\n",
    "\n",
    "dim_cities.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Load data to S3 Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fact_immigration_df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"i94yr\", \"i94mon\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(output_data +'fact_immigration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_airport.write.option(\"header\",True) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(output_data +'dim_airport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_immigrants.write.option(\"header\",True) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(output_data +'dim_immigrants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_cities.write.option(\"header\",True) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(output_data +'dim_cities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def records_count_check(df_list):\n",
    "    for df in df_list:\n",
    "        dfname =[x for x in globals() if globals()[x] is df][0]\n",
    "        count = df.count()\n",
    "        if count < 1:\n",
    "            print(\"ERROR: Table has 0 records\")\n",
    "        else:\n",
    "            print(f\"COUNT CHECKED: Table {dfname} has {count} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def col_dtype_check(df, cols, col_type):\n",
    "    for col in cols:\n",
    "        if dict(df.dtypes)[col] == col_type:\n",
    "            print(f\"CORRECT DATA TYPE: column {col} checked! Type {col_type}\")\n",
    "        else:\n",
    "            print(f\"FAILED DATA TYPE CHECK: column {col} has wrong dtype: {col_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT CHECKED: Table dim_airport has 54968 records\n",
      "COUNT CHECKED: Table dim_cities has 2891 records\n"
     ]
    }
   ],
   "source": [
    "records_count_check([dim_airport, dim_cities])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT DATA TYPE: column ArrivalDate checked! Type timestamp\n",
      "CORRECT DATA TYPE: column DepartureDate checked! Type timestamp\n"
     ]
    }
   ],
   "source": [
    "# print(dict(fact_immigration_df.dtypes))\n",
    "col_dtype_check(fact_immigration_df, ['ArrivalDate','DepartureDate'],'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create table/view for querying in SQL\n",
    "fact_immigration_df.createOrReplaceTempView(\"fact_immigration_table\")\n",
    "dim_immigrants.createOrReplaceTempView(\"dim_immigrants_table\")\n",
    "dim_airport.createOrReplaceTempView(\"dim_airport_table\")\n",
    "dim_cities.createOrReplaceTempView(\"dim_cities_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                name|count(1)|\n",
      "+--------------------+--------+\n",
      "|dim_immigrants_table|     100|\n",
      "|   dim_airport_table|   54968|\n",
      "|fact_immigration_...|     100|\n",
      "|    dim_cities_table|    2891|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count records of each table\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \"fact_immigration_table\" as name, COUNT(*)\n",
    "FROM fact_immigration_table\n",
    "UNION\n",
    "SELECT \"dim_immigrants_table\" as name, COUNT(*)\n",
    "FROM dim_immigrants_table\n",
    "UNION\n",
    "SELECT \"dim_airport_table\" as name, COUNT(*)\n",
    "FROM dim_airport_table\n",
    "UNION\n",
    "SELECT \"dim_cities_table\" as name, COUNT(*)\n",
    "FROM dim_cities_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|DestinationState|count(1)|\n",
      "+----------------+--------+\n",
      "|              NJ|      15|\n",
      "|              NY|      14|\n",
      "|              MI|      12|\n",
      "|              FL|      12|\n",
      "|              MA|      11|\n",
      "|              TX|       5|\n",
      "|              NH|       4|\n",
      "|              CA|       4|\n",
      "|            null|       3|\n",
      "|              CT|       3|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Do some analysis queries\n",
    "\n",
    "#Query 1: TOP STATE with most immigration\n",
    "spark.sql(\"\"\"\n",
    "SELECT f.DestinationState, count(*)\n",
    "FROM fact_immigration_table f\n",
    "GROUP BY f.DestinationState\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|i94mon|count(1)|\n",
      "+------+--------+\n",
      "|   4.0|     100|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query 2: TOP MONTHs with most Immigration application\n",
    "spark.sql(\"\"\"\n",
    "SELECT f.i94mon, count(*)\n",
    "FROM fact_immigration_table f\n",
    "GROUP BY f.i94mon\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+\n",
      "| age|gender|count(1)|\n",
      "+----+------+--------+\n",
      "|55.0|  null|       3|\n",
      "|40.0|     M|       2|\n",
      "|57.0|  null|       2|\n",
      "|49.0|     F|       2|\n",
      "|55.0|     M|       2|\n",
      "|48.0|     M|       2|\n",
      "|66.0|  null|       2|\n",
      "|61.0|     F|       2|\n",
      "|27.0|     M|       2|\n",
      "|58.0|     M|       2|\n",
      "+----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query 3: Immigration applicants by age and gender\n",
    "spark.sql(\"\"\"\n",
    "SELECT i.age, i.gender, count(*)\n",
    "FROM dim_immigrants_table i\n",
    "GROUP BY i.age, i.gender\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " 1. fact_immigration:\n",
    " \n",
    " - cicid : unique ID of each profile\n",
    " - i94yr: year of application\n",
    " - i94mon: month of application\n",
    " - CountryID: Country ID of applicants\n",
    " - PortID: Port ID of applicants\n",
    " - ArrivalDate: Date applicants arrive to US\n",
    " - DepartureDate: Date applicants depart from US\n",
    " - DestinationState: State applicants arrive to\n",
    " - i94mode: how did the applicant arrived in the USA\n",
    " - VisaCat: Visa category of applicant\n",
    " - visatype: Visa type of applicant\n",
    " \n",
    " \n",
    " 2. dim_airport:\n",
    " \n",
    " - StateID: State ID of a specific airport\n",
    " - type: Type of airport\n",
    " - name: Name of airport\n",
    " - iso_country: Country that airport belongs to\n",
    " - gps_code: GPS Code of airport\n",
    " - coordinate: Latitude and longtitute of an airport\n",
    " \n",
    "3.dim_immigrants:\n",
    " \n",
    "  - cicid: unique ID of each profile\n",
    "  - Age: Age of applicant\n",
    "  - BirthYear: Birth year of applicant\n",
    "  - gender: gender of applicant\n",
    "  \n",
    "  \n",
    "4. dim_cities:\n",
    "\n",
    " - State Code: State code\n",
    " - State: State name\n",
    " - City: city name\n",
    " - Median Age: median age of a city\n",
    " - Total Population: total population of a city\n",
    " - Foreign-born: number of foreign-born of a city\n",
    " - Race: majority race of a city\n",
    " - Count: number of people of a specific race of a city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### The choice of tools, technologies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Main file data format is SAS and expect to be grow in the future so we will use Spark cluster to ingest and transform data. Spark is a general-purpose distributed data processing engine that is suitable for use in a wide range of circumstances. Sparks's in-memory processing saves a lot of time and makes it easier and efficient. It's also easy to scale up Spark cluster for larger data volume in the future\n",
    "- After data is transformed, to support prompt writing and reading, Spark will load processed data into S3 Data Lake, data are queried in-place on the S3 parquet data. S3 is considered as a data lake with cheaper cost than a data warehouse such as Redshift, easily accessible and integrated with other platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Jars required for Spark to connect to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "aws-java-sdk-1.7.4.2.jar\n",
    "hadoop-aws-2.7.3.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### How often the data should be updated: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Data should be updated **monthly** for the fact table as shown in the documentation for the I-94 Arrivals Program data\n",
    "- Meanwhile for dimension table, data can be updated upon request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### In case the data was increased by 100x:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "-  Then the Spark cluster that is hosted on an EMR instance should be scaled up accordingly to help process the data properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### In case the data populates a dashboard that must be updated on a daily basis by 7am every day:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "- A Schedule job tool such as Apache Airflow will help. This requires set up a DAG that trigger on scheduled time of daily 7am to run Python scripts and Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### In case the database needed to be accessed by 100+ people: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "  * As we use S3 to store data directly and query on fly, 100 or more people can easily access the data by assigning for all of them the proper IAM roles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
